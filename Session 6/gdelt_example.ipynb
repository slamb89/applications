{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adopted from GDELT Data Wrangle by James Houghton https://nbviewer.jupyter.org/github/JamesPHoughton/Published_Blog_Scripts/blob/master/GDELT%20Wrangler%20-%20Clean.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional GDELT resources: \n",
    "    \n",
    "    GDELT library overview: https://colab.research.google.com/drive/1rnKEHKV1StOwGtFPsCctKDPTBB_kHOc_?usp=sharing \n",
    "    \n",
    "    GDELT with big data: https://github.com/linwoodc3/gdeltPyR/wiki/Pulling-Large-GDELT-Data\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART I: Get GDELT DATA FOR NIGER\n",
    "\n",
    "\n",
    "### Get the GDELT index files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import lxml.html as lh\n",
    "\n",
    "gdelt_base_url = 'http://data.gdeltproject.org/events/'\n",
    "\n",
    "# get the list of all the links on the gdelt file page\n",
    "page = requests.get(gdelt_base_url+'index.html') #Grab GDELT reference list which is by day\n",
    "doc = lh.fromstring(page.content)\n",
    "link_list = doc.xpath(\"//*/ul/li/a/@href\") #Returns all the possible CSV files of GDELT data as a references list\n",
    "\n",
    "# separate out those links that begin with four digits \n",
    "'''\n",
    "Will extract just the days resulting in list like: \n",
    "['20200617.export.CSV.zip',\n",
    " '20200616.export.CSV.zip',\n",
    " '20200615.export.CSV.zip',...]\n",
    " Until 2015\n",
    "'''\n",
    "\n",
    "file_list = [x for x in link_list if str.isdigit(x[0:4])]\n",
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counters to help assess how many files are coming and going out\n",
    "infilecounter = 0\n",
    "outfilecounter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uses GDELT Index file list to download GDELT data for that day for that country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path #To help navigate the file directories\n",
    "import urllib #To request from GDELT\n",
    "import zipfile #TO unzip the files we downlaod\n",
    "import glob #To go through multiple files in a directory\n",
    "import operator \n",
    "\n",
    "local_path = './results/' # Will save to empy results folder to help keep file clean\n",
    "\n",
    "fips_country_code = 'NG'  ## !!!!! THIS IS THE NIGER COUNTRY CODE GETS ONLY NIGER DATA!!!!\n",
    "\n",
    "#Adjust list number to get days wanted \n",
    "for compressed_file in file_list[:7]: #!!!!!Only getting index 0 to 6!!!!!!\n",
    "    print(compressed_file,)\n",
    "    \n",
    "    # if we dont have the compressed file stored locally, go get it. Keep trying if necessary.\n",
    "    while not os.path.isfile(local_path+compressed_file): \n",
    "        print('downloading,'),\n",
    "        urllib.request.urlretrieve(url=gdelt_base_url+compressed_file, \n",
    "                           filename=local_path+compressed_file)\n",
    "        \n",
    "    # extract the contents of the compressed file to a temporary directory    \n",
    "    print('extracting,'),\n",
    "    z = zipfile.ZipFile(file=local_path+compressed_file, mode='r')    \n",
    "    z.extractall(path=local_path+'tmp/')\n",
    "    \n",
    "    # parse each of the csv files in the working directory, \n",
    "    print('parsing,'),\n",
    "    for infile_name in glob.glob(local_path+'tmp/*'):\n",
    "        outfile_name = local_path+fips_country_code+'%04i.tsv'%outfilecounter\n",
    "        \n",
    "        # open the infile and outfile\n",
    "        with open(infile_name, mode='r', encoding=\"ISO-8859-1\") as infile, open(outfile_name, mode='w') as outfile:\n",
    "            for line in infile:\n",
    "                # extract lines with our interest country code\n",
    "                if fips_country_code in operator.itemgetter(51, 37, 44)(line.split('\\t')):    \n",
    "                    outfile.write(line)\n",
    "            outfilecounter +=1\n",
    "            \n",
    "        # delete the temporary file\n",
    "        os.remove(infile_name)\n",
    "    infilecounter +=1\n",
    "    print('done', infilecounter)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART II:  PARSE DATA AGAIN\n",
    "\n",
    "### Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get the GDELT field names from a helper file\n",
    "colnames = pd.read_csv('CSV.header.fieldids.csv')['Field Name']\n",
    "\n",
    "\n",
    "# Build DataFrames from each of the intermediary files\n",
    "files = glob.glob(local_path+fips_country_code+'*')\n",
    "DFlist = []\n",
    "for active_file in files:\n",
    "    print(active_file)\n",
    "    DFlist.append(pd.read_csv(active_file, sep='\\t', header=None, dtype=str,\n",
    "                              names=colnames, index_col=['GLOBALEVENTID'], encoding='iso-8859-1'))\n",
    "\n",
    "# Merge the file-based dataframes and save a pickle\n",
    "DF = pd.concat(DFlist)\n",
    "DF.to_pickle(local_path+'backup'+fips_country_code+'.pickle')    \n",
    "    \n",
    "# once everythin is safely stored away, remove the temporary files\n",
    "for active_file in files:\n",
    "    os.remove(active_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "Niger_Data = pd.read_pickle(r\"./results/backupNG.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See top 5 lines of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Niger_Data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function  to turn codebooks  into look up tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ref_dict(df):\n",
    "    cols = list(df)\n",
    "    ref_dict = {}\n",
    "    for row in df.iterrows(): \n",
    "        ref_dict[row[1][cols[0]]] = row[1][cols[1]]\n",
    "    \n",
    "    return ref_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert each codebook and store in object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in event codes\n",
    "eventCodes = ref_dict(pd.read_csv(\"./Ref Codes/CAMEO.eventcodes.txt\", sep='\\t'))\n",
    "#Read in Goldsteinscale\n",
    "goldScale = ref_dict(pd.read_csv(\"./Ref Codes/CAMEO.goldsteinscale.txt\", sep='\\t'))\n",
    "#Read in ethnic groups\n",
    "ethnicCodes =ref_dict(pd.read_csv(\"./Ref Codes/CAMEO.ethnic.txt\", sep='\\t'))\n",
    "#Read in known Groups\n",
    "knownGroups = ref_dict(pd.read_csv(\"./Ref Codes/CAMEO.knowngroup.txt\", sep='\\t'))\n",
    "#Read in relgion\n",
    "religionCodes = ref_dict(pd.read_csv(\"./Ref Codes/CAMEO.religion.txt\", sep='\\t'))\n",
    "#Read in type\n",
    "typeCodes = ref_dict(pd.read_csv(\"./Ref Codes/CAMEO.type.txt\", sep='\\t'))\n",
    "\n",
    "eventCodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn colnames into list for ref\n",
    "\n",
    "cross_ref = list(colnames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create look up table to get values instead of numbers\n",
    "\n",
    "look_up_code = {\"eventCodes\": [26,27,28], \"goldScale\":[30], \"ethnicCodes\":[9,19], \"knownGroups\":[8,18], \n",
    "                \"religionCodes\":[10,11,20,21], \"typeCodes\":[12,13,14,22,23,24]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Helper function to user can reorient data based on interest from codes\n",
    "\n",
    "data: Niger_Data - pandas dataframe\n",
    "ref: key value from look_look_code - string\n",
    "codebook: reference \n",
    "'''\n",
    "\n",
    "import math\n",
    "\n",
    "def search_dict(data,ref, codebook):\n",
    "    res = {}\n",
    "    look_up = look_up_code[ref]\n",
    "    col_names = []\n",
    "    for i in look_up: \n",
    "        col_names.append(cross_ref[i])\n",
    "    \n",
    "    for col in col_names: \n",
    "        for row in data.iterrows(): \n",
    "            if isinstance(row[1][col],float):\n",
    "                #print (type(row[1][col]), col)\n",
    "                pass\n",
    "            else: \n",
    "                #print (col)\n",
    "                var = codebook[row[1][col]].upper()\n",
    "                #print (var, row[1][col])\n",
    "                if var in res.keys(): \n",
    "                    #print(row[1][col])\n",
    "                    res[var].append(dict(row[1]))\n",
    "                else: \n",
    "                    res[var] = [dict(row[1])]\n",
    "    return res\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = search_dict(Niger_Data, \"ethnicCodes\", ethnicCodes)\n",
    "res.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verfication to ensure code is working properly\n",
    "for k,v in res.items(): \n",
    "    print (k, \": \", len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put each collection of articles in a Dataframe\n",
    "list_res = []\n",
    "\n",
    "for cat in res.values(): \n",
    "    #print(cat)\n",
    "    list_res.append(pd.DataFrame(cat))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_res[3] #access the group you are interested in by changing the variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework 4: Do some type of analysis with GDELT data. It can be country focused (e.g. Guatemala) or topic focused (e.g. attacks or bilateral agreements)\n",
    "\n",
    "### Must write in the first cell what you are interested in. Code must work but results can be garabage. Update the GDELT parameters to get the information you want and then include some type of plot can be a graph or can be a map.  \n",
    "\n",
    "### Total Points Possible 19\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
